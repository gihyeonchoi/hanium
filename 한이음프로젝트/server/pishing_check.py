# https://www.bugbountyclub.com/blog/view/14

# import sys
# sys.executable
import csv
from urllib.parse import urlparse
import re

def URL_parsing(text):
    """
    ë©”ì‹œì§€ì—ì„œ URLë§Œ ì¶”ì¶œ (ìŠ¤í‚´ ì—†ìŒ, user@host í¬í•¨)
    - http, https ìŠ¤í‚´
    - ìŠ¤í‚´ ì—†ëŠ” ë„ë©”ì¸
    - user@host í”¼ì‹± ì¼€ì´ìŠ¤
    """
    print(f"í…ìŠ¤íŠ¸ : {text}")

    # ìŠ¤í‚´ í¬í•¨: http://, https://
    # user:pass@host í¬ë§·ë„ í¬í•¨ë˜ë„ë¡ [^\s]+
    scheme_pattern = r'https?://[^\s]+'

    # ìŠ¤í‚´ ì—†ëŠ” but user@host í”¼ì‹±ìš©: [domain]@[something]
    user_at_host_pattern = r'[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}@[^\s/]+'

    # ì¼ë°˜ ìŠ¤í‚´ ì—†ëŠ” ë„ë©”ì¸ + ì„ íƒì  ê²½ë¡œ
    bare_domain_pattern = r'(?:www\.)?[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s]*)?'

    # 3ê°œ íŒ¨í„´ ì¡°í•©
    url_pattern = f'({scheme_pattern}|{user_at_host_pattern}|{bare_domain_pattern})'

    urls = re.findall(url_pattern, text)
    urls = list(set(urls))  # ì¤‘ë³µ ì œê±°

    print("ê²€ìƒ‰í•œ URL:", urls)
    return urls

def URL_check(url):
    '''
    URLì„ ë°›ì•„ ë„ë©”ì¸ DBì™€ ëŒ€ì¡°. Bool ë¦¬í„´. URLì€ String ìœ¼ë¡œ
    www. ì—¬ë¶€ ê´€ê³„ì—†ì´ ë§¤ì¹­ ì‹œ True
    '''
    import csv
    from urllib.parse import urlparse

    def get_base_domain(u):
        parsed = urlparse(u)
        return parsed.netloc.lower() if parsed.netloc else parsed.path.lower()

    # DB ë¶ˆëŸ¬ì˜¤ê¸°
    with open('../domains_data/all_subdomains_response.csv', 'r', encoding='utf-8') as f:
        rdr = csv.reader(f)
        db_domains = set(row[0].lower() for row in rdr if row)

    urls_to_try = []

    # ìŠ¤í‚´ì´ ìˆëŠ” ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ
    if url.startswith(('http://', 'https://')):
        urls_to_try.append(url)
    else:
        # ìŠ¤í‚´ì´ ì—†ëŠ” ê²½ìš° https â†’ http ìˆœìœ¼ë¡œ ì‹œë„
        urls_to_try.append('https://' + url)
        urls_to_try.append('http://' + url)

    for try_url in urls_to_try:
        base_url = get_base_domain(try_url)

        url_variants = {base_url}
        if base_url.startswith("www."):
            url_variants.add(base_url[4:])
        else:
            url_variants.add("www." + base_url)

        print(f"â–¶ ì‹œë„ ì¤‘: {try_url} â†’ ë„ë©”ì¸ ë³€í˜•: {url_variants}")

        for variant in url_variants:
            if variant in db_domains:
                print(f"âœ… URL_check: {variant} ì°¾ìŒ")
                return True

    print(f"âŒ URL_check: ëª¨ë“  í”„ë¡œí† ì½œ ì‹œë„ ì‹¤íŒ¨ - {url}")
    return False



def SSL_check(url): 
    '''
    URLì„ ë°›ì•„ SSL ì¸ì¦ì„œ ë¹„êµ.
    - 1 = ì •ìƒ (SSL ì¸ì¦ì„œ ìœ íš¨)
    - 0 = ë¹„ì •ìƒ (SSL ì˜¤ë¥˜)
    - -1 = HTTP ì—°ê²° (SSL ì—†ìŒ)
    - -2 = ì—°ê²° ì‹¤íŒ¨ (ë„ë©”ì¸ ì—†ìŒ, ì—°ê²° ê±°ë¶€ ë“±)
    '''
    import requests
    import certifi
    from urllib.parse import urlparse

    print(f"ssl ì²´í¬ : {url}")

    def try_request(full_url):
        try:
            response = requests.get(full_url, verify=certifi.where(), timeout=5)
            print(f"[ì„±ê³µ] {full_url} ì—°ê²°ë¨ (status: {response.status_code})")
            return 1  # SSL ì¸ì¦ ì„±ê³µ
        except requests.exceptions.SSLError:
            print(f"[SSL ì˜¤ë¥˜] {full_url}")
            return 0  # SSL ì¸ì¦ ì‹¤íŒ¨
        except requests.exceptions.RequestException as e:
            print(f"[ìš”ì²­ ì‹¤íŒ¨] {full_url} - {e}")
            return -2  # ì—°ê²° ì‹¤íŒ¨

    parsed = urlparse(url)
    # ìŠ¤í‚´ì´ ì—†ë‹¤ë©´ https ë¨¼ì € ë¶™ì—¬ì„œ ì‹œë„
    if not parsed.scheme:
        https_url = "https://" + url
        result = try_request(https_url)
        if result == -2:  # https ì‹¤íŒ¨í•˜ë©´ httpë„ ì‹œë„
            http_url = "http://" + url
            print(f"https ì‹¤íŒ¨, http ì‹œë„ ì¤‘: {http_url}")
            return -1 if try_request(http_url) == 1 else -2
        else:
            return result

    # ìŠ¤í‚´ì´ ëª…ì‹œëœ ê²½ìš° ê·¸ëŒ€ë¡œ ì‹œë„
    if parsed.scheme == "https":
        return try_request(url)
    elif parsed.scheme == "http":
        print("HTTP URLì…ë‹ˆë‹¤. SSL ì¸ì¦ì„œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return -1
    else:
        print(f"[ì˜¤ë¥˜] ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤í‚´: {parsed.scheme}")
        return -2



def Domain_check(url):
    '''
    URLì„ ë°›ì•„ ìƒì„±ëœ ë‚ ì§œì™€ í˜„ì¬ ë‚ ì§œ ì°¨ì´ë¥¼ ì¼(day)ë¡œ ë¦¬í„´.
    '''
    from datetime import datetime
    import whois

    try:
        domain_info = whois.whois(url)
        creation_date = domain_info.creation_date

        # ë¦¬ìŠ¤íŠ¸ë¡œ ì˜¬ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
        if isinstance(creation_date, list):
            creation_date = creation_date[0]

        if creation_date is None:
            print("ë„ë©”ì¸ ìƒì„±ì¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return -1  # í˜¹ì€ None

        day = datetime.now() - creation_date
        return day.days

    except Exception as e:
        print(f"Domain_check ì˜¤ë¥˜: {e}")
        return -1  # ì˜¤ë¥˜ ì‹œ -1 ë°˜í™˜




def Location_to_IP(url):
    '''
    URLì„ ë°›ì•„ ISO êµ­ê°€ëª…ì„ ë¦¬í„´.
    ë¦¬í„´ê°’ 2ê°œ ì˜ˆì‹œ : ì •ìƒë¹„ì •ìƒ bool , ë‚˜ë¼ì´ë¦„ string
    '''
    import socket
    import geoip2.database
    # GeoLite2 ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    country_reader = geoip2.database.Reader("GeoLite2-Country.mmdb")

    # IP ì£¼ì†Œ ì–»ê¸°
    def get_ip_from_url(url):
        try:
            ip_address = socket.gethostbyname(url)
            return ip_address
        except socket.gaierror:
            return None

    # CSV íŒŒì¼ì—ì„œ ISO ì½”ë“œ -> í•œê¸€ êµ­ê°€ëª… ë”•ì…”ë„ˆë¦¬ ë§Œë“¤ê¸°
    def load_country_dict(csv_file):
        iso_to_korean = {}
        with open(csv_file, mode='r', encoding='utf-8') as file:
            reader = csv.DictReader(file)
            for row in reader:
                korean_name = row['í•œê¸€ëª…']
                iso_code = row['2ìë¦¬ì½”ë“œ'].upper()
                iso_to_korean[iso_code] = korean_name
        return iso_to_korean

    print(f"ë¡œì¼€ì´ì…˜ ì•„ì´í”¼ : {url}")
    try:
        parsed_url = urlparse(url)
        print(parsed_url)
        ip_address = get_ip_from_url(parsed_url.netloc if parsed_url.netloc else parsed_url.path)
        if ip_address is None:
            print("IP ì£¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False, "ì•Œìˆ˜ì—†ìŒ"
        
        # ISO ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
        country_response = country_reader.country(ip_address)
        print("country_response : ", country_response)
        iso_code = (country_response.registered_country.iso_code or "").upper()
        print(f"[Location] ISO ì½”ë“œ: {iso_code}")


        # í•œê¸€ êµ­ê°€ëª… ë§¤í•‘
        country_dict = load_country_dict('../ISO_code.csv')
        korean_name = country_dict.get(iso_code, "ì•Œ ìˆ˜ ì—†ìŒ")
        print("êµ­ê°€ëª… (í•œê¸€):", korean_name)
        return True, korean_name

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)
        return False, "ì•Œìˆ˜ì—†ìŒ"
    
def Additional_risk(url):
    '''
    URL ë¬¸ìì—´ì—ì„œ ì¶”ê°€ì ì¸ ìœ„í—˜ ìš”ì†Œ íƒì§€
    - "//"ê°€ URL ê²½ë¡œ ì¤‘ê°„ì— ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•  ê²½ìš°
    - "@" ì‹¬ë³¼ì´ í¬í•¨ëœ ê²½ìš° (í”¼ì‹± URLì— ì¢…ì¢… ì‚¬ìš©ë¨)
    - XSS ê³µê²© íŒ¨í„´ íƒì§€
    - Open Redirect íŒ¨í„´ íƒì§€
    
    ë¦¬í„´: ìœ„í—˜ë„ ì ìˆ˜, íƒì§€ëœ ìœ„í—˜ ìš”ì†Œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    '''
    import re
    from urllib.parse import urlparse, parse_qs, unquote
    
    risk_messages = []
    risk_level = 0
    
    # URL ë””ì½”ë”© (ì¸ì½”ë”©ëœ ì•…ì„± íŒ¨í„´ íƒì§€ë¥¼ ìœ„í•´)
    decoded_url = unquote(url.lower())
    print(f"Additional_risk url : {url}")
    # 1. ê¸°ì¡´ íŒ¨í„´ ê²€ì‚¬
    # ì¤‘ê°„ì— //ê°€ 1ë²ˆ ì´ìƒ ë°˜ë³µë˜ëŠ” ê²½ìš° (ë‹¨, ì‹œì‘ì˜ https:// ì œì™¸)
    if url.count('//') > 1:
        risk_messages.append('ğŸš¨ URLì— ë¹„ì •ìƒì ì¸ ë¦¬ë‹¤ì´ë ‰ì…˜ ê²½ë¡œ(//)ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.')
        risk_level += 60
    
    # @ ê¸°í˜¸ í¬í•¨ ì—¬ë¶€
    if '@' in url:
        risk_messages.append('ğŸš¨ URLì— ë¹„ì •ìƒì ì¸ ì¸ì¦ ìš°íšŒ(@)ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.')
        risk_level += 60
    
    # 2. Open Redirect íŒ¨í„´ ê²€ì‚¬
    open_redirect_params = [
        'url', 'redirect', 'return', 'next', 'goto', 'target', 'destination',
        'redir', 'redirect_url', 'return_url', 'go', 'out', 'link', 'continue',
        'site', 'view', 'to', 'ref', 'jump', 'jumpto', 'forward', 'dest'
    ]
    
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query.lower())
        
        for param in open_redirect_params:
            if param in params:
                param_value = params[param][0] if params[param] else ''
                # ì™¸ë¶€ URLë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸í•˜ëŠ” íŒ¨í„´ ê²€ì‚¬
                if any(pattern in param_value for pattern in ['http://', 'https://', 'ftp://', '//']):
                    risk_messages.append(f'ğŸš¨ Open Redirect ì·¨ì•½ì ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤ ({param})')
                    risk_level = 100  # Open RedirectëŠ” ë§¤ìš° ìœ„í—˜
                    break
    except:
        pass
    
    # 3. XSS ê³µê²© íŒ¨í„´ ê²€ì‚¬
    xss_patterns = [
        r'<script[^>]*>',  # <script> íƒœê·¸
        r'javascript:',     # javascript: í”„ë¡œí† ì½œ
        r'onerror\s*=',    # onerror ì´ë²¤íŠ¸
        r'onclick\s*=',    # onclick ì´ë²¤íŠ¸
        r'onload\s*=',     # onload ì´ë²¤íŠ¸
        r'onmouseover\s*=', # onmouseover ì´ë²¤íŠ¸
        r'<iframe[^>]*>',  # iframe íƒœê·¸
        r'<img[^>]*onerror', # img íƒœê·¸ì™€ onerror
        r'alert\s*\(',     # alert í•¨ìˆ˜
        r'eval\s*\(',      # eval í•¨ìˆ˜
        r'expression\s*\(', # CSS expression
        r'vbscript:',      # vbscript í”„ë¡œí† ì½œ
        r'data:.*base64',  # data URL scheme
        r'<svg[^>]*onload', # SVG íƒœê·¸
        r'&#x[0-9a-fA-F]+;', # 16ì§„ìˆ˜ HTML ì—”í‹°í‹°
        r'&#[0-9]+;',      # 10ì§„ìˆ˜ HTML ì—”í‹°í‹°
        r'document\.cookie', # ì¿ í‚¤ ì ‘ê·¼
        r'window\.location', # ìœ„ì¹˜ ë³€ê²½
        r'<object[^>]*>',  # object íƒœê·¸
        r'<embed[^>]*>',   # embed íƒœê·¸
    ]
    
    for pattern in xss_patterns:
        if re.search(pattern, decoded_url, re.IGNORECASE):
            risk_messages.append(f'ğŸš¨ XSS ê³µê²© íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤: {pattern}')
            risk_level = 100  # XSSëŠ” ë§¤ìš° ìœ„í—˜
            break
    
    # 4. ë™í˜• ë¬¸ì ê³µê²© (Homograph Attack) ê²€ì‚¬ - í‚¤ë¦´ ë¬¸ì ë“±
    # ì˜ì–´ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ ë¬¸ìì¸ ê²½ìš° íƒì§€
    cyrillic_chars = {
        'Ğ°': 'a', 'Ğµ': 'e', 'Ğ¾': 'o', 'Ñ€': 'p', 'Ñ': 'c', 'Ñƒ': 'y', 'Ñ…': 'x',
        'Ğ': 'A', 'Ğ’': 'B', 'Ğ•': 'E', 'Ğš': 'K', 'Ğœ': 'M', 'Ğ': 'H', 'Ğ': 'O', 
        'Ğ ': 'P', 'Ğ¡': 'C', 'Ğ¢': 'T', 'Ğ£': 'Y', 'Ğ¥': 'X'
    }
    
    # ê·¸ë¦¬ìŠ¤ ë¬¸ì
    greek_chars = {
        'Î±': 'a', 'Î¿': 'o', 'Î½': 'v', 'Ï„': 't', 'Ï': 'p', 'Î¼': 'u',
        'Î‘': 'A', 'Î’': 'B', 'Î•': 'E', 'Î–': 'Z', 'Î—': 'H', 'Î™': 'I', 
        'Îš': 'K', 'Îœ': 'M', 'Î': 'N', 'ÎŸ': 'O', 'Î¡': 'P', 'Î¤': 'T', 'Î¥': 'Y', 'Î§': 'X'
    }
    
    # ê¸°íƒ€ ìœ ì‚¬ ë¬¸ì (ë¼í‹´ í™•ì¥ ë“±)
    lookalike_chars = {
        'É‘': 'a', 'Ï²': 'c', 'Ô': 'd', 'Ò½': 'e', 'Ö': 'g', 'Ò»': 'h', 'Ñ–': 'i',
        'Ñ˜': 'j', 'Ó': 'l', 'Õ¸': 'n', 'Ö…': 'o', 'Ö„': 'q', 'Ñ•': 's', 'Õ½': 'u',
        'Ñµ': 'v', 'Ô': 'w', 'Ñ…': 'x', 'Ñƒ': 'y', 'á´¢': 'z'
    }
    
    # ë„ë©”ì¸ ë¶€ë¶„ ì¶”ì¶œ
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # ë™í˜• ë¬¸ì íƒì§€
        homograph_found = False
        found_chars = []
        
        for char in domain:
            if char in cyrillic_chars:
                found_chars.append(f'{char}(í‚¤ë¦´ë¬¸ìâ†’{cyrillic_chars[char]})')
                homograph_found = True
            elif char in greek_chars:
                found_chars.append(f'{char}(ê·¸ë¦¬ìŠ¤ë¬¸ìâ†’{greek_chars[char]})')
                homograph_found = True
            elif char in lookalike_chars:
                found_chars.append(f'{char}(ìœ ì‚¬ë¬¸ìâ†’{lookalike_chars[char]})')
                homograph_found = True
        
        if homograph_found:
            risk_messages.append(f'ğŸš¨ ë„ë©”ì¸ì— ì˜ì–´ë¡œ ìœ„ì¥í•œ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {", ".join(found_chars[:3])}{"..." if len(found_chars) > 3 else ""}')
            risk_level = 100
    except:
        pass
    
    # 5. ì¶”ê°€ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ê²€ì‚¬
    suspicious_patterns = [
        (r'[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}', 'IP ì£¼ì†Œê°€ ì§ì ‘ ì‚¬ìš©ë¨'),
        (r'(bit\.ly|tinyurl|goo\.gl|short\.link)', 'ë‹¨ì¶• URL ì„œë¹„ìŠ¤ ì‚¬ìš©'),
        (r'[0-9a-f]{32,}', 'ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í•´ì‹œê°’ í¬í•¨'),
        (r'\.tk|\.ml|\.ga|\.cf', 'ë¬´ë£Œ ë„ë©”ì¸ ì‚¬ìš©'),
        (r'(%[0-9a-fA-F]{2}){5,}', 'ê³¼ë„í•œ URL ì¸ì½”ë”©'),
    ]
    
    for pattern, message in suspicious_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            risk_messages.append(f'âš ï¸ {message}')
            risk_level += 20
    
    # 6. ë‹¤ì¤‘ ë¦¬ë‹¤ì´ë ‰ì…˜ ì²´ì¸ ê²€ì‚¬
    redirect_count = len(re.findall(r'(https?://|ftp://|//)', decoded_url))
    if redirect_count > 2:  # í”„ë¡œí† ì½œ í¬í•¨í•´ì„œ 2ê°œ ì´ìƒ
        risk_messages.append(f'ğŸš¨ ë‹¤ì¤‘ ë¦¬ë‹¤ì´ë ‰ì…˜ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ ({redirect_count}ê°œì˜ URL)')
        risk_level = 100
    
    # ìœ„í—˜ë„ ìµœëŒ€ê°’ ì œí•œ
    risk_level = min(100, risk_level)
    
    # ìœ„í—˜ë„ê°€ 100ì¸ ê²½ìš° í™•ì • ë©”ì‹œì§€ ì¶”ê°€
    if risk_level == 100 and not any('í”¼ì‹± í™•ì •' in msg for msg in risk_messages):
        risk_messages.insert(0, 'ğŸš¨ ì´ URLì€ í”¼ì‹± ì‚¬ì´íŠ¸ì¼ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!')
    
    return risk_level, risk_messages
